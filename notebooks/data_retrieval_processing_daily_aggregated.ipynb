{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for retrieving, processing, and saving daily (aggregated) UK rainfall data via the EA API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, date\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date and time: 2023-05-21 20:54:22\n"
     ]
    }
   ],
   "source": [
    "# printing current date and time\n",
    "print('Date and time: ' + str(datetime.today().replace(microsecond=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list present working directory\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data retrieval functions - only for stations and measures via the API\n",
    "\n",
    "# function to retrieve data on rainfall stations\n",
    "def get_data_stations(parameter_name = None,\n",
    "                 parameter = None,\n",
    "                 qualifier = None,\n",
    "                 label = None,\n",
    "                 town = None,\n",
    "                 catchment_name = None,\n",
    "                 river_name = None,\n",
    "                 station_reference = None,\n",
    "                 rloi_id = None,\n",
    "                 search = None,\n",
    "                 lat = None,\n",
    "                 long = None,\n",
    "                 d = None,\n",
    "                 type = None,\n",
    "                 status = None):\n",
    "    \"\"\"Get details of rainfall monitoring stations from the EA API\n",
    "    \n",
    "      Query parameter details:\n",
    "    \n",
    "      :param parameter_name: Return only those stations which measure parameters with the given name, for example Rainfall, Water Level or Flow.\n",
    "      :param parameter: Return only those stations which measure parameters with the given short form name, for example rainfall, level or flow.\n",
    "      :param qualifier: Return only those stations which measure parameters with qualifier. Useful qualifiers are Stage and Downstream Stage (for stations such as weirs which measure levels at two locations), Groundwater for groundwater levels as opposed to river levels and Tidal Level for tidal levels.\n",
    "      :param label: Return only those stations whose label is exactly as given.\n",
    "      :param town: Return only those stations whose town is as given. Not all stations have an associated town.\n",
    "      :param catchment_name: Return only those stations whose catchment name is exactly as given. Not all stations have an associated catchment area.\n",
    "      :param river_name: Return only those stations whose river name is exactly as given. Not all stations have an associated river name.\n",
    "      :param station_reference: Return only those stations whose reference identifier is as given. The station reference is an internal identifier used by the Environment Agency.\n",
    "      :param rloi_id: Return only the station (if there is one) whose RLOIid (River Levels on the Internet identifier) matches.\n",
    "      :param search: Return only those stations whose label contains the given value.\n",
    "      :param lat: Return those stations whose location falls within d km of the given latitude/longitude (in WGS84 coordinates), this may be approximated by a bounding box.\n",
    "      :param long: Return those stations whose location falls within d km of the given latitude/longitude (in WGS84 coordinates), this may be approximated by a bounding box.\n",
    "      :param d: Return those stations whose location falls within d km of the given latitude/longitude (in WGS84 coordinates), this may be approximated by a bounding box.\n",
    "      :param type: Return only those stations of the given type, where type can be one of \"SingleLevel\", \"MultiTraceLevel\", \"Coastal\", \"Groundwater\" or \"Meteorological\"\n",
    "      :param status: Return only those stations with the given status. Can be one of \"Active\", \"Closed\" or \"Suspended\".\n",
    "      \"\"\"\n",
    "    \n",
    "    # URL of the UK EA API\n",
    "    api_url = \"https://environment.data.gov.uk/flood-monitoring/id/stations\"\n",
    " \n",
    "    # build a dictionary of the query parameters\n",
    "    params =  {'parameterName': parameter_name,\n",
    "                'parameter': parameter,\n",
    "                'qualifier': qualifier,\n",
    "                'label': label,\n",
    "                'town': town,\n",
    "                'catchmentName': catchment_name,\n",
    "                'riverName': river_name,\n",
    "                'stationReference': station_reference,\n",
    "                'RLOIid': rloi_id,\n",
    "                'search': search,\n",
    "                'lat': lat,\n",
    "                'long': long,\n",
    "                'dist': d,\n",
    "                'type': type,\n",
    "                'status': status\n",
    "              }\n",
    "    \n",
    "    # getting data on stations from the EA API\n",
    "    response = requests.get(api_url, \n",
    "                            params = params)\n",
    "\n",
    "    # ensuring that cookies are allowed\n",
    "    #response = requests.get(api_url, params, cookies=response.cookies)\n",
    "    \n",
    "    # extracting JSON data from the response container\n",
    "    data = response.json()\n",
    "\n",
    "    # extracting the items elements and loading data to a pandas data frame\n",
    "    stations = pd.DataFrame(data[\"items\"])\n",
    "\n",
    "    return(stations)\n",
    "\n",
    "\n",
    "# function to retrieve data on rainfall measures\n",
    "def get_data_measures(parameter_name = None,\n",
    "                     parameter = None,\n",
    "                     qualifier = None,\n",
    "                     station_reference = None,\n",
    "                     station = None,\n",
    "                     search = None):\n",
    "    \"\"\"Get details of measures available from the EA API\n",
    "\n",
    "       :param parameter_name: Return only measures for parameters with the given name, for example Water Level or Flow.\n",
    "       :param parameter: Return only measures for parameters with the given short form name, for example level or flow.\n",
    "       :param qualifier: Return only those measures with qualifier. Useful qualifiers are Stage and Downstream Stage (for stations such as weirs which measure levels at two locations), Groundwater for groundwater levels as opposed to river levels and Tidal Level for tidal levels.\n",
    "       :param station_reference: Return only those measures which are available from the station with the given reference identifier.\n",
    "       :param station: Return only those measures which are available from the station with the given URI.\n",
    "       :param search: Return only those measures whose label contains the given value.\n",
    "    \"\"\"\n",
    "    api_url = \"http://environment.data.gov.uk/flood-monitoring/id/measures\"\n",
    "    \n",
    "     # build a dictionary of the query parameters\n",
    "    params = {'parameterName': parameter_name,\n",
    "            'parameter': parameter,\n",
    "            'qualifier': qualifier,\n",
    "            'stationReference': station_reference,\n",
    "            'station': station,\n",
    "            'search': search\n",
    "            }\n",
    "    \n",
    "    # getting data about measures from the EA API\n",
    "    response = requests.get(api_url, \n",
    "                            params)\n",
    "\n",
    "    # ensuring that cookies are allowed\n",
    "    #response = requests.get(api_url, params, cookies=response.cookies)\n",
    "    \n",
    "    # extracting JSON data from the response container\n",
    "    data = response.json()\n",
    "\n",
    "    # extracting the items elements and loading data to a pandas data frame\n",
    "    measures = pd.DataFrame(data[\"items\"])\n",
    "\n",
    "    return(measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving the rainfall stations and measures data\n",
    "rainfall_stations_data = get_data_stations(parameter='rainfall')\n",
    "rainfall_measures_data = get_data_measures(parameter='rainfall')\n",
    "\n",
    "# retrieving the rainfall readings archive for today\n",
    "rainfall_readings_today = pd.read_csv('http://environment.data.gov.uk/flood-monitoring/data/readings.csv?parameter=rainfall&_view=full&date=' + str(date.today()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of station data: (997, 19)\n",
      "Shape of measures data: (1023, 13)\n",
      "Shape of today's rainfall readings data: (57515, 13)\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of entries for each dataset\n",
    "print('Shape of station data: ' + str(rainfall_stations_data.shape))\n",
    "print('Shape of measures data: ' + str(rainfall_measures_data.shape))\n",
    "print(\"Shape of today's rainfall readings data: \" + str(rainfall_readings_today.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in the rainfall stations data: 2\n",
      "Number of duplicates in the rainfall measures data: 25\n",
      "Shape of datasets after dropping duplicates:\n",
      "Shape of station data: (995, 19)\n",
      "Shape of measures data: (998, 13)\n"
     ]
    }
   ],
   "source": [
    "# checking for duplicates in the first two data frames by 'station_id' or 'stationReference' and dropping them\n",
    "print('Number of duplicates in the rainfall stations data: ' + str(rainfall_stations_data[rainfall_stations_data.duplicated(['stationReference'])].shape[0]))\n",
    "print('Number of duplicates in the rainfall measures data: ' + str(rainfall_measures_data[rainfall_measures_data.duplicated(['stationReference'])].shape[0]))\n",
    "\n",
    "# dropping the duplicates and retaining the first values - this can be investigated later and rows with more entries can be\n",
    "# kept to retain the most amount of information\n",
    "rainfall_stations_data.drop_duplicates(subset=['stationReference'], keep='first', inplace = True)\n",
    "rainfall_measures_data.drop_duplicates(subset=['stationReference'], keep='first', inplace = True)\n",
    "\n",
    "# resetting the indices of the data frames\n",
    "rainfall_stations_data.reset_index(drop = True, inplace = True)\n",
    "rainfall_measures_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Checking the number of entries for each dataset after dropping duplicates\n",
    "print('Shape of datasets after dropping duplicates:')\n",
    "print('Shape of station data: ' + str(rainfall_stations_data.shape))\n",
    "print('Shape of measures data: ' + str(rainfall_measures_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating data by date and stationReference or station_id for all station IDs\n",
    "rainfall_readings_aggregated_values = rainfall_readings_today.groupby(['date','stationReference'], as_index=False)['value'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(997, 33)\n"
     ]
    }
   ],
   "source": [
    "# merging the 3 dataframes for rainfall stations, measures, and daily reaadings\n",
    "\n",
    "# left join of readings with measures to keep all readings and outer join of resulting df with stations to keep\n",
    "# details of non-responsive stations as well\n",
    "rainfall_readings_aggregated = rainfall_readings_aggregated_values.merge(rainfall_measures_data,on='stationReference', how='left').merge(rainfall_stations_data,on='stationReference', how='outer')\n",
    "\n",
    "# viewing the shape of the resulting df\n",
    "print(rainfall_readings_aggregated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the combined dataframe:(997, 16)\n"
     ]
    }
   ],
   "source": [
    "# further data cleaning\n",
    "\n",
    "# dropping duplicate and redundant columns\n",
    "rainfall_readings_aggregated = rainfall_readings_aggregated.drop(columns=['@id_x',\n",
    "                                                              'label_x',\n",
    "                                                              'measures',\n",
    "                                                              'latestReading',\n",
    "                                                              'notation_x',\n",
    "                                                              'station',\n",
    "                                                              '@id_y',\n",
    "                                                              'notation_y',\n",
    "                                                              # columns relevant only for river data\n",
    "                                                              'catchmentName',\n",
    "                                                              'dateOpened',\n",
    "                                                              'riverName',\n",
    "                                                              'stageScale',\n",
    "                                                              'status',\n",
    "                                                              'town',\n",
    "                                                              'wiskiID',\n",
    "                                                              'datumOffset',\n",
    "                                                              'RLOIid'\n",
    "                                                             ])\n",
    "\n",
    "# adding date column\n",
    "\n",
    "# creating date variable\n",
    "rainfall_readings_aggregated['date'] = str(date.today())\n",
    "\n",
    "# renaming columns\n",
    "rainfall_readings_aggregated.rename(columns = {\n",
    "                                           'value': 'reading_value',\n",
    "                                           'parameter': 'parameter_id',\n",
    "                                           'parameterName': 'parameter_name',\n",
    "                                           'period': 'reading_period',\n",
    "                                           'qualifier': 'reading_qualifier',\n",
    "                                           'stationReference': 'station_id',\n",
    "                                           'unit':'reading_unit_id',\n",
    "                                           'unitName':'reading_unit_name',\n",
    "                                           'valueType':'reading_value_type',\n",
    "                                           'easting':'station_easting',\n",
    "                                           'northing': 'station_northing',\n",
    "                                           'gridReference': 'station_grid_reference',\n",
    "                                           'label_y': 'station_type',\n",
    "                                           'lat': 'station_latitude',\n",
    "                                           'long':'station_longitude',\n",
    "                                           'valueType':'reading_value_type',\n",
    "                                           'gridReference': 'station_grid_reference'},\n",
    "                              inplace = True)\n",
    "                                                           \n",
    "# reordering columns\n",
    "rainfall_readings_aggregated = rainfall_readings_aggregated[['date',\n",
    "                                                    'station_id',\n",
    "                                                    'station_type',\n",
    "                                                    'station_grid_reference',\n",
    "                                                    'station_latitude',\n",
    "                                                    'station_longitude',\n",
    "                                                    'station_easting',\n",
    "                                                    'station_northing',\n",
    "                                                    'parameter_id',\n",
    "                                                    'parameter_name',\n",
    "                                                    'reading_qualifier',\n",
    "                                                    'reading_value',\n",
    "                                                    'reading_unit_id',\n",
    "                                                    'reading_unit_name',\n",
    "                                                    'reading_value_type',\n",
    "                                                    'reading_period'\n",
    "                                               ]]\n",
    "\n",
    "# viewing the shape of the resulting combined df\n",
    "print(\"Shape of the combined dataframe:\" + str(rainfall_readings_aggregated.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__date__: date  \n",
    "__time__: time  \n",
    "__station_id__: station identifier  \n",
    "__station_type__: type of station  \n",
    "__station_grid_reference__: grid reference for the station, rounded to a 100m grid  \n",
    "__station_latitude__: latitude coordinates of station  \n",
    "__station_longitude__: longitude coordinates of station  \n",
    "__station_easting__: easting coordinates of station  \n",
    "__station_northing__: northing coordinates of station  \n",
    "__parameter_id__: short name/id of the quantity being measured  \n",
    "__parameter_name__: name of the quantity being measured  \n",
    "__reading_qualifier__: a qualifier for the quantity being measured, \"Tipping Bucket Raingauge\" for rainfall  \n",
    "__reading_value__: the value of the reading for the associated measurement  \n",
    "__reading_unit_id__: unit id/url for the reading  \n",
    "__reading_unit_name__: unit name for the reading  \n",
    "__reading_value_type__: type of measurement, e.g., total, mean, etc.  \n",
    "__reading_period__: the period between successive readings, in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stations with no recent rainfall measurements: 92\n",
      "\n",
      "IDs of the respective stations:\n",
      "\n",
      "['4163' 'E1310' '0' 'E1179' '1117' '45101' '270400TP' '055223' 'E15520'\n",
      " 'E1965' 'E11060' '008632' '000032' 'E23633' '1760' '023839' 'E23535'\n",
      " '019356' 'E43041' '266474TP' '577805' 'E42971' 'E22876' 'E11040'\n",
      " '000102TP' '49106' '592848' '4103' '550148' 'E21637' '586493' '1795'\n",
      " 'E23657' '073422' '50108' '1409' 'E7120' 'E22735' '238097TP' '593321'\n",
      " 'E2450' '562992' 'E1691' '45157' '007533' '584098' 'E14880' '46101'\n",
      " 'E2859' '476898_TG_333' '246847TP' 'E24775' '021028' 'E6780' '568363'\n",
      " 'E1928' '585022' 'E24585' '605382' '563599' '55000A' 'E24141' '562811'\n",
      " 'E24499' 'E5720' '45108_' 'E1711' '4527' '6666LO' '1409Lucas' '4549'\n",
      " '4548' 'E5650' '7015' '1607' '47176' '1338' '1750' '1810' '1408' 'Coldw1'\n",
      " 'Mitch1' 'Edgeh1' 'Dubbs1' '585122_' '2102SO'\n",
      " '575092______________________________________'\n",
      " '574786______________________________________' 'E84360' '062916' 'E60880'\n",
      " 'NE063']\n"
     ]
    }
   ],
   "source": [
    "# check if there are any stations without recent rainfall measurements and list their Station IDs\n",
    "\n",
    "# list total stations with no recent rainfall measurements\n",
    "print( \"Total stations with no recent rainfall measurements: \" + str(rainfall_readings_aggregated['reading_value'].isna().sum()))\n",
    "\n",
    "print('')\n",
    "\n",
    "print('IDs of the respective stations:')\n",
    "\n",
    "print('')\n",
    "\n",
    "# listing their IDs\n",
    "print(rainfall_readings_aggregated[rainfall_readings_aggregated['reading_value'].isna() == True]['station_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN value check with an assert statement\n",
    "#assert len(rainfall_readings_aggregated[rainfall_readings_aggregated['reading_value'].isna() == True]['station_id'].tolist()) == 0, f\"0 expected, got: {len(rainfall_readings_aggregated[rainfall_readings_aggregated['reading_value'].isna() == True]['station_id'].tolist())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stations without latitude coordinates: 8\n",
      "\n",
      "IDs of the respective stations:\n",
      "\n",
      "['024121' '068416' '282947TP' 'E23500' 'E7040' '6666LO' '4549' '4548']\n"
     ]
    }
   ],
   "source": [
    "# listing stations without latitude (can also be checked for longitude)\n",
    "print( \"Total stations without latitude coordinates: \" + str(rainfall_readings_aggregated['station_latitude'].isna().sum()))\n",
    "\n",
    "print('')\n",
    "\n",
    "print('IDs of the respective stations:')\n",
    "\n",
    "print('')\n",
    "\n",
    "# listing their IDs\n",
    "print(rainfall_readings_aggregated[rainfall_readings_aggregated['station_latitude'].isna() == True]['station_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in the combined data frame: 0\n",
      "Shape of combined data after dropping duplicates: (997, 16)\n"
     ]
    }
   ],
   "source": [
    "# check for duplicate stations by station ID on combined data frame (they could have come from the responses df)\n",
    "print('Number of duplicates in the combined data frame: ' + str(rainfall_readings_aggregated[rainfall_readings_aggregated.duplicated(['station_id'])].shape[0]))\n",
    "\n",
    "# dropping the duplicates and retaining the first values - this can be investigated later and rows with more entries can be\n",
    "# kept to retain the most amount of information\n",
    "rainfall_readings_aggregated.drop_duplicates(subset=['station_id'], keep='first', inplace = True)\n",
    "\n",
    "# resetting the indices of the data frames\n",
    "rainfall_readings_aggregated.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Checking the number of entries for the dataset after dropping duplicates\n",
    "print('Shape of combined data after dropping duplicates: ' + str(rainfall_readings_aggregated.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving combined dataset as csv and excel files\n",
    "rainfall_readings_aggregated.to_csv(f'../data/rainfall_data_aggregated_{date.today()}.csv')\n",
    "rainfall_readings_aggregated.to_excel(f'../data/rainfall_data_aggregated_{date.today()}.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
